{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6110b5",
   "metadata": {},
   "source": [
    "### Data source: https://www.bracs.icar.cnr.it/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd0608",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import timm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "# Load config\n",
    "preproc_conf = OmegaConf.load(\"../conf/preproc.yaml\")\n",
    "preproc_conf = preproc_conf['classic_mil_on_embeddings_bag']['bracs_224_224_patches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9f33b-7b50-4a8f-a02d-49f2998015a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDANUM = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c4c9b",
   "metadata": {},
   "source": [
    "### Locate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = preproc_conf.img_dir_lvl4\n",
    "parent_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3fafd-2bf5-4349-a032-6f22857fc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(preproc_conf.weights_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992d6e7",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48778317",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train = np.load( parent_folder+'bracs_level4_regions_224_training_data_macenkonorm_bracs.npy')\n",
    "y_train = np.load( parent_folder+'bracs_level4_regions_224_training_label.npy')\n",
    "\n",
    "x_val = np.load( parent_folder+'bracs_level4_regions_224_validation_data_macenkonorm_bracs.npy')\n",
    "y_val = np.load( parent_folder+'bracs_level4_regions_224_validation_label.npy')\n",
    "\n",
    "x_test = np.load( parent_folder+'bracs_level4_regions_224_test_data_macenkonorm_bracs.npy')\n",
    "y_test = np.load( parent_folder+'bracs_level4_regions_224_test_label.npy')\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f898cfa",
   "metadata": {},
   "source": [
    "### Preprocess label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13173567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary encode\n",
    "lb = LabelEncoder()\n",
    "#lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "label_oh = lb.transform(y_train)\n",
    "\n",
    "y_train_oh = lb.transform(y_train)\n",
    "y_val_oh = lb.transform(y_val)\n",
    "y_test_oh = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_oh.shape, y_test_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca040cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f92d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(label_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730e153",
   "metadata": {},
   "source": [
    "### Create balanced data loader -> balanced folds with subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc0bb5-b507-49fc-a5ff-211c38b418c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_biopsy_subset(labels, minority_class_ratio=0.2, rnd_seed=38):\n",
    "    # set random seed as given\n",
    "    np.random.seed(rnd_seed)\n",
    "    \n",
    "    # collect selected biopsies that will be in the balanced subset\n",
    "    test_local_idx = []\n",
    "    \n",
    "    # get current class occurences for biopsy\n",
    "    class_occurence = np.array(list(dict( Counter(labels) ).values()))[ np.argsort(list(dict( Counter(labels) ).keys()))]\n",
    "    #print(class_occurence)\n",
    "    \n",
    "    # calc class weights\n",
    "    class_weights = ( class_occurence / class_occurence.sum() ).astype(np.float32)\n",
    "    class_weights_dict = dict( zip( np.arange(class_weights.shape[0]), class_weights ))\n",
    "    #print(class_weights_dict)\n",
    "    \n",
    "    # how many of biopsies to include in the balanced subset\n",
    "    nr_class_test = int(labels.shape[0]*np.min(class_weights)*minority_class_ratio)\n",
    "\n",
    "    # collect biopsy indices for the balanced subset\n",
    "    for s in np.unique(labels): #loop over labelss\n",
    "        s_idx = np.arange(labels.shape[0])[labels == s]\n",
    "        rnd_idx = np.random.permutation(s_idx.shape[0])\n",
    "        test_local_idx.append(s_idx[rnd_idx[:nr_class_test]])\n",
    "\n",
    "    # aggregate all the balanced subset's indices\n",
    "    test_idx = np.concatenate(test_local_idx)\n",
    "    \n",
    "    # other indices not in balanced set will be the rest\n",
    "    train_idx = np.arange(labels.shape[0])[~np.in1d(np.arange(labels.shape[0]), test_idx)]\n",
    "    \n",
    "    return train_idx, test_idx#, label_remaining[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91207994-c98a-465a-83e3-45054944eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_biopsy_subset(labels, minority_class_ratio=0.2, rnd_seed=38):\n",
    "    # set random seed as given\n",
    "    np.random.seed(rnd_seed)\n",
    "    \n",
    "    # collect selected biopsies that will be in the balanced subset\n",
    "    test_local_idx = []\n",
    "    \n",
    "    # get current class occurences for biopsy\n",
    "    class_occurence = np.array(list(dict( Counter(labels) ).values()))[ np.argsort(list(dict( Counter(labels) ).keys()))]\n",
    "    #print(class_occurence)\n",
    "    \n",
    "    # calc class weights\n",
    "    class_weights = ( class_occurence / class_occurence.sum() ).astype(np.float32)\n",
    "    class_weights_dict = dict( zip( np.arange(class_weights.shape[0]), class_weights ))\n",
    "    #print(class_weights_dict)\n",
    "    \n",
    "    # how many of biopsies to include in the balanced subset\n",
    "    nr_class_test = int(labels.shape[0]*np.min(class_weights)*minority_class_ratio)\n",
    "\n",
    "    # collect biopsy indices for the balanced subset\n",
    "    for s in np.unique(labels): #loop over labelss\n",
    "        s_idx = np.arange(labels.shape[0])[labels == s]\n",
    "        rnd_idx = np.random.permutation(s_idx.shape[0])\n",
    "        test_local_idx.append(s_idx[rnd_idx[:nr_class_test]])\n",
    "\n",
    "    # aggregate all the balanced subset's indices\n",
    "    test_idx = np.concatenate(test_local_idx)\n",
    "    \n",
    "    # other indices not in balanced set will be the rest\n",
    "    train_idx = np.arange(labels.shape[0])[~np.in1d(np.arange(labels.shape[0]), test_idx)]\n",
    "    \n",
    "    return train_idx, test_idx#, label_remaining[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4753cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_back_balanced_training_fold( X_current, y_current,\n",
    "                                      minority_class_ratio=1.0, rnd_seed=12 ):\n",
    "    \n",
    "    _, test_idx, = create_biopsy_subset(y_current,\n",
    "                                                 minority_class_ratio,\n",
    "                                                 rnd_seed)\n",
    "    X_train_balanced = X_current[test_idx]\n",
    "    y_train_balanced = y_current[test_idx]\n",
    "    #y_train_balanced_oh = lb.transform(y_train_balanced)\n",
    "    #print( X_train_balanced.shape, y_train_balanced_oh.shape )\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec6a3",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionsDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 num_classes, \n",
    "                 transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {'image': image,\n",
    "                'label': label\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/pytorch/issues/7359\n",
    "class BalancedSampler(Sampler):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 num_classes, \n",
    "                 transform=None,\n",
    "                 rand_seed=12):\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        self.rand_seed = rand_seed\n",
    "        \n",
    "\n",
    "        \n",
    "    def give_back_balanced_training_fold(self, X_current, y_current, minority_class_ratio, rand_seed):\n",
    "        _, test_idx = create_biopsy_subset(y_current, minority_class_ratio, rand_seed)\n",
    "        \n",
    "        return test_idx\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        test_idx = self.give_back_balanced_training_fold(self.data, self.labels, minority_class_ratio=1.0, rand_seed=self.rand_seed)\n",
    "        \n",
    "        return len(test_idx) #3220\n",
    "\n",
    "    def __iter__(self):\n",
    "        test_idx = self.give_back_balanced_training_fold(self.data, self.labels, minority_class_ratio=1.0, rand_seed=self.rand_seed)\n",
    "        \n",
    "        random.shuffle(test_idx)\n",
    "        num_batches = len(test_idx) // self.batch_size - 1 #99\n",
    "        \n",
    "        n=0\n",
    "        while num_batches > 0:\n",
    "            \n",
    "            sampled = test_idx[n*32:(n+1)*32]\n",
    "                \n",
    "            yield sampled #(32, 3, 224, 224)\n",
    "            num_batches -=1\n",
    "            n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchBalancedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, sampler, batch_size):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _, idx in enumerate(iter(self.sampler)): #99\n",
    "            batch = idx\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler) // self.batch_size - 1 #99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc7e9f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7027b-d10b-42be-ac43-af917404f389",
   "metadata": {},
   "source": [
    "#### very small LR for backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3aa46-153a-42c8-bc17-4c3dff90c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "\n",
    "# alternative\n",
    "# model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e78da7-7cac-47d2-9eb0-66f448ee1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bc523-bcc2-493b-8408-14fa4471c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HERE THIS IS SIMPLEHEAD !  ## CHOOSE ##\n",
    "model.fc = nn.Linear(in_features=2048, out_features=7)\n",
    "\n",
    "\n",
    "## HERE THIS IS COMPLEXHEAD !  ## CHOOSE ##\n",
    "\"\"\"\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=2048, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Dropout(p=0.5),\n",
    "    \n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Linear(in_features=128, out_features=7),\n",
    "\n",
    "    #nn.Softmax(1)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bb9f3-9f4c-4b90-92c7-06e3322dde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of parameters: one for the backbone and one for the head\n",
    "backbone_params = [param for name, param in model.named_parameters() if 'fc' not in name]\n",
    "head_params = model.fc.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d95085",
   "metadata": {},
   "source": [
    "## Traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder = preproc_conf.weights_dir+'weights_train_resnet50_smallLRbackbone_simplehead_level4_macenko_bracs_50epochs/'\n",
    "\n",
    "os.makedirs(weights_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "os.makedirs(weights_folder, exist_ok=True)\n",
    "\n",
    "def train_model(model,\n",
    "                device,\n",
    "                transform,\n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                num_epochs):\n",
    "    \n",
    "    history = {}\n",
    "    history_train_loss = []\n",
    "    history_train_acc = []\n",
    "    history_val_loss = []\n",
    "    history_val_acc = []\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    train_dataset = CollectionsDataset(data=x_train,\n",
    "                                       labels=y_train_oh,\n",
    "                                       num_classes=NUM_CLASSES,\n",
    "                                       transform=transform)\n",
    "    \n",
    "    # VAL dataset\n",
    "    val_dataset = CollectionsDataset(data=x_val,\n",
    "                                     labels=y_val_oh,\n",
    "                                     num_classes=NUM_CLASSES,\n",
    "                                     transform=transform)\n",
    "\n",
    "    # create the pytorch data loader\n",
    "    val_dataset_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # training loop wiht balanced folds\n",
    "    for epoch in range(0, num_epochs):\n",
    "        \n",
    "        sampler = BalancedSampler(\n",
    "                     batch_size=32,\n",
    "                     data=x_train,\n",
    "                     labels=y_train_oh,\n",
    "                     num_classes=NUM_CLASSES, \n",
    "                     transform=None,\n",
    "                     rand_seed=int(epoch*1.5+3*epoch))\n",
    "\n",
    "        batch_sampler = BatchBalancedSampler(sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "        # create the pytorch data loader\n",
    "        train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           num_workers=4,\n",
    "                                                           batch_sampler=batch_sampler)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Iterate over data.\n",
    "        \n",
    "        \n",
    "        for bi, d in enumerate(train_dataset_loader):\n",
    "            \n",
    "            inputs = d[\"image\"]\n",
    "            labels = d[\"label\"]\n",
    "            inputs = inputs.to(device, dtype=torch.float, non_blocking=True)\n",
    "            labels = labels.to(device, dtype=torch.long,  non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            #acc metrics\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        checkpoint = { \n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            #'scheduler': scheduler\n",
    "        }\n",
    "            \n",
    "        epoch_loss = running_loss / len(sampler) #(len(sampler) -52)\n",
    "        writer.add_scalar(\"Train CE Loss\", epoch_loss)\n",
    "        print('Train CE Loss: {:.4f}'.format(epoch_loss), f'----- Train Accuracy: {100 * correct // total} %')\n",
    "        \n",
    "        \n",
    "        history_train_loss.append(epoch_loss)\n",
    "        history_train_acc.append(100 * correct // total)\n",
    "        \n",
    "        \n",
    "        # VALIDATION\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        running_loss_val = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for bi, d in enumerate(val_dataset_loader):\n",
    "                inputs = d[\"image\"]\n",
    "                labels = d[\"label\"]\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = model(inputs)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                \n",
    "                loss_val = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss_val += loss_val.item() * inputs.size(0)\n",
    "                \n",
    "        epoch_loss_val = running_loss_val / x_val.shape[0]\n",
    "        print('Val CE Loss: {:.4f}'.format(epoch_loss_val), f'----- Val Accuracy: {100 * correct_val // total_val} %')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        history_val_loss.append(epoch_loss_val)\n",
    "        history_val_acc.append(100 * correct_val // total_val)\n",
    "        \n",
    "        if (100 * correct_val // total_val) > 44:\n",
    "            print(weights_folder+f'checkpoint_epoch_{epoch}'+\\\n",
    "                   '_{:.4f}'.format(epoch_loss_val)+f'_{100 * correct_val // total_val} %.pth')\n",
    "            torch.save(checkpoint, weights_folder+f'checkpoint_epoch_{epoch}'+\\\n",
    "                   '_{:.4f}'.format(epoch_loss_val)+f'CE_{100 * correct_val // total_val}_acc.pth')\n",
    "\n",
    "    history['train_loss'] = history_train_loss\n",
    "    history['train_acc'] = history_train_acc\n",
    "    history['val_loss'] = history_val_loss\n",
    "    history['val_acc'] = history_val_acc\n",
    "    \n",
    "    writer.flush()\n",
    "        \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4600c",
   "metadata": {},
   "source": [
    "## Transforms, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96396a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_transform = transforms.Compose([transforms.ToTensor(), transforms.RandomResizedCrop(size=224, scale=(0.55,1))])\n",
    "\n",
    "dummy_transform2 = transforms.Compose([transforms.ToTensor(), transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)])\n",
    "\n",
    "dummy_transform3 = transforms.Compose([transforms.ToTensor(), transforms.RandomAdjustSharpness(sharpness_factor=3, p=0.5)])\n",
    "\n",
    "dummy_transform4 = transforms.Compose([transforms.ToTensor(), transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 3))])\n",
    "\n",
    "\n",
    "dummy_dataset = CollectionsDataset(data=x_train,\n",
    "                                       labels=y_train_oh,\n",
    "                                       num_classes=7,\n",
    "                                       transform=dummy_transform4)\n",
    "\n",
    "plt.imshow(np.swapaxes(dummy_dataset[0]['image'],0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36139c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = CollectionsDataset(data=x_train,\n",
    "                                       labels=y_train_oh,\n",
    "                                       num_classes=7,\n",
    "                                       transform=None)\n",
    "plt.imshow(dummy_dataset[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264acdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch\n",
    "\n",
    "# define some re-usable stuff\n",
    "IMAGE_SIZE = 224\n",
    "NUM_CLASSES = 7\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(f'cuda:{CUDANUM}')\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # IMAGENET !\n",
    "     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.RandomResizedCrop(size=224, scale=(0.55,1)),\n",
    "     #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "     transforms.RandomAdjustSharpness(sharpness_factor=3, p=0.5),\n",
    "     transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 3))\n",
    "    ])\n",
    "\n",
    "# push model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7db106",
   "metadata": {},
   "source": [
    "### Train UNI with balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f6e12",
   "metadata": {},
   "source": [
    "## Optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad374e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=7):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device=torch.device(f'cuda:{CUDANUM}' if torch.cuda.is_available() else \"cpu\") \n",
    "    if device.type == f'cuda:{CUDANUM}':\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32575b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a very small learning rate for the backbone and a higher one for the head\n",
    "optimizer_ft = optim.Adam([\n",
    "    {'params': backbone_params, 'lr': 1e-5, 'amsgrad': True},  \n",
    "    {'params': head_params, 'lr': 1e-4, 'amsgrad': True}      \n",
    "])\n",
    "\n",
    "lr_sch = None # lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.8) # was 10 and 0.8\n",
    "device = torch.device(f'cuda:{CUDANUM}' if torch.cuda.is_available() else 'cpu') #device config\n",
    "\n",
    "seed_torch() # FIX SEED \n",
    "model_ft, history = train_model(model,\n",
    "                       device,\n",
    "                       train_transform,\n",
    "                       optimizer_ft,\n",
    "                       lr_sch,\n",
    "                       num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1b919-3028-451f-8d73-12e405e83922",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot( np.arange(len(history['train_loss'])), history['train_loss'], label='train' )\n",
    "plt.plot( np.arange(len(history['val_loss'])), history['val_loss'], label='val' )\n",
    "plt.legend(fontsize=18)\n",
    "plt.xlabel('epochs', fontsize=22)\n",
    "plt.ylabel('Cross entropy loss', fontsize=22)\n",
    "plt.tick_params(labelsize=18)\n",
    "#plt.yscale('log')\n",
    "plt.title('mcr=0.5, bs=32, AdamW, lr=1e-4, StepLR(step_size=5, gamma=0.8)')\n",
    "#plt.savefig('test3_aug3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a93db",
   "metadata": {},
   "source": [
    "### Save plot data as well !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcbde4e-6445-4eea-87d8-dab1101dcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{weights_folder}history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540e0b7-2393-44bd-a40d-3a1fe2aa1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2b974-2b2b-4f33-a941-acdab279f22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
