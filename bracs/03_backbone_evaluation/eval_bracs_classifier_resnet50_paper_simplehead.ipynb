{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc3d42-b25f-4c55-bf29-1e6abff7cf21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# System and Utility Libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# Scientific and Data Processing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import to_rgba, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning and Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import timm\n",
    "\n",
    "# Metrics and Evaluation Libraries\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, recall_score, f1_score, \n",
    "    precision_recall_curve, average_precision_score, \n",
    "    precision_score, cohen_kappa_score, confusion_matrix, \n",
    "    accuracy_score, classification_report, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer, LabelEncoder\n",
    "\n",
    "# Configuration Libraries\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Filter Warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846fe59-6d92-48b8-8e5c-af99ca54e4ad",
   "metadata": {},
   "source": [
    "### Config and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0609b3-9287-4810-8979-75e08eb4c654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preproc_conf = OmegaConf.load(\"../conf/preproc.yaml\")\n",
    "preproc_conf = preproc_conf['classic_mil_on_embeddings_bag']['bracs_224_224_patches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9677c4-db01-4a31-9747-88fc75099d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0' \n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43822c8a-c190-4eb0-8a11-cbaea4705679",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = preproc_conf.img_dir_lvl4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6709183-225f-4c83-933b-4086c4d42f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_val = np.load( parent_folder+'bracs_level4_regions_224_test_data_macenkonorm_bracs.npy')\n",
    "y_val = np.load( parent_folder+'bracs_level4_regions_224_test_label.npy')\n",
    "\n",
    "#Binary encode\n",
    "lb = LabelEncoder()\n",
    "lb.fit(y_val)\n",
    "y_val_oh = lb.transform(y_val)\n",
    "X_val.shape, y_val.shape, y_val_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b852d1-7f90-40c7-a322-1d0abb108399",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3ef0c-feeb-4c0c-b831-d2de6d94a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original array\n",
    "lb_classes = lb.classes_\n",
    "\n",
    "# Desired abbreviations\n",
    "desired_order = ['N', 'PB', 'UDH', 'ADH', 'FEA', 'DCIS', 'IC']\n",
    "\n",
    "# Dictionary mapping full class names to abbreviations\n",
    "class_mapping = {\n",
    "    'N': 'NORMAL',\n",
    "    'PB': 'PATHOLOGICAL-BENIGN',\n",
    "    'UDH': 'UDH',\n",
    "    'ADH': 'ADH',\n",
    "    'FEA': 'FEA',\n",
    "    'DCIS': 'DCIS',\n",
    "    'IC': 'INVASIVE-CARCINOMA'\n",
    "}\n",
    "\n",
    "# Reverse mapping to go from full name to abbreviation\n",
    "reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "# Convert lb_classes to abbreviations using the reverse mapping\n",
    "lb_classes_abbr = np.array([reverse_mapping[cls] for cls in lb_classes])\n",
    "\n",
    "# Get the indices that would sort the array based on desired order\n",
    "sorted_indices = np.argsort([desired_order.index(abbr) for abbr in lb_classes_abbr])\n",
    "\n",
    "# Display the sorted indices\n",
    "sorted_indices, lb_classes_abbr[sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccbd85-8292-4cd0-afde-75a7548f1b92",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b211f2-5800-4833-8c94-b16f44035afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a309c47-7fe4-4b28-829c-cf53fc49453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c836ca-c3f8-4976-bbcc-31a14c61da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(in_features=2048, out_features=7)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0185943-e759-43ca-b868-0310e19acb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_transforms(mean = (0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    t = transforms.Compose(\n",
    "                        [transforms.ToTensor(),\n",
    "                         transforms.Normalize(mean = mean, std = std)])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657892f-e588-48de-8095-6d242fd1893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_to_use = preproc_conf.weights_dir+'weights_train_resnet50_smallLRbackbone_simplehead_level4_macenko_bracs_50epochs/checkpoint_epoch_33_1.4022CE_49_acc.pth'\n",
    "\n",
    "loaded_model = torch.load( weight_to_use, map_location=torch.device('cuda'))\n",
    "model.load_state_dict(loaded_model['model'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c59ea-2810-4e65-9f19-d7d2838487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch tensor for the images\n",
    "imgs_tensor = torch.zeros((X_val.shape[0], X_val.shape[3], X_val.shape[2], X_val.shape[1]), dtype=torch.float32)\n",
    "\n",
    "# Apply transforms to images\n",
    "for i in range(X_val.shape[0]):\n",
    "    imgs_tensor[i] = default_transforms()(X_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c584a-234c-4d17-a50e-3e97691fcf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bracs_class_pred_val_all = []\n",
    "\n",
    "# Create a data loader for the images\n",
    "data_loader = DataLoader( imgs_tensor, batch_size=128, shuffle=False, num_workers=1)\n",
    "\n",
    "# Process the images using the feature extractor\n",
    "for img in data_loader:\n",
    "    img = img.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bracs_class_pred_val = torch.nn.functional.softmax( model(img), dim=1 ) # softmax applied here !\n",
    "\n",
    "    bracs_class_pred_val_all.append(bracs_class_pred_val.cpu().numpy())\n",
    "bracs_class_pred_val_all =  np.concatenate( bracs_class_pred_val_all )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d974d-5db0-49ad-80ee-877a9631d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bracs_class_pred_val_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133426fd-b382-4f5b-a9f2-3cf6785a6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report( y_val_oh, np.argmax( bracs_class_pred_val_all, axis=1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82582c6b-71d8-4b1e-a95d-50e1957a41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Predictions/Predictions_weights_train_resnet50_simplehead_level4_macenkonorm_bracs_50epochs_checkpoint_epoch_33_1.4022CE_49_acc.npy', bracs_class_pred_val_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493eca48-7c47-40d4-b656-413e627b90aa",
   "metadata": {},
   "source": [
    "### Rename variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15014939-e5b3-4197-95a4-b4d07e2af539",
   "metadata": {},
   "outputs": [],
   "source": [
    "biopsy_df_local_test = y_val_oh\n",
    "final_pred_ensemble_local_test = bracs_class_pred_val_all\n",
    "\n",
    "y_true = biopsy_df_local_test\n",
    "predicted_labels = np.argmax( final_pred_ensemble_local_test, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55d044-3569-4648-9fbe-e9ba7743b32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "biopsy_df_local_test.shape, y_true.shape, predicted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb70090-8396-4e54-b791-fa1e3a3f3340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_true, final_pred_ensemble_local_test, multi_class=\"ovr\", average=\"macro\")\n",
    "print(\"ROC macro:\", roc_auc)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, final_pred_ensemble_local_test, multi_class=\"ovr\", average=\"weighted\")\n",
    "print(\"ROC weighted:\", roc_auc)\n",
    "\n",
    "f1 = f1_score(y_true, predicted_labels, average='macro')\n",
    "print(\"F1 macro:\", f1)\n",
    "\n",
    "f1 = f1_score(y_true, predicted_labels, average='weighted')\n",
    "print(\"F1 weighted:\", f1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, predicted_labels)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576074d-b680-4c86-87ce-0c9d00ea9cea",
   "metadata": {},
   "source": [
    "### metrics on test - bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717c30-358c-418c-a569-2ffc3109fb21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_bootstrap_samples(y_true, y_pred, num_bootstrap_samples=10000, random_seed=42):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    n = len(y_true)\n",
    "    bootstrap_samples = []\n",
    "\n",
    "    for _ in tqdm(range(num_bootstrap_samples)):\n",
    "        while True:\n",
    "            # Create bootstrap sample\n",
    "            indices = np.random.randint(0, n, n)\n",
    "            sample_y_true = y_true[indices]\n",
    "            sample_y_pred = y_pred[indices]\n",
    "            \n",
    "            # Check if all labels are present for macro/weighted metrics\n",
    "            if len(np.unique(sample_y_true)) == len(np.unique(y_true)):\n",
    "                # Check if valid one-vs-rest samples exist for each class\n",
    "                valid_sample = all(\n",
    "                    np.sum(sample_y_true == class_index) > 0 and np.sum(sample_y_true != class_index) > 0\n",
    "                    for class_index in range(7)\n",
    "                )\n",
    "                if valid_sample:\n",
    "                    bootstrap_samples.append((sample_y_true, sample_y_pred))\n",
    "                    break  # Exit the loop if all conditions are met\n",
    "\n",
    "    return bootstrap_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726aee6-b29c-47cd-ad58-ef8f6ffbc158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bootstrap_samples = generate_bootstrap_samples(\n",
    "    y_true, \n",
    "    final_pred_ensemble_local_test, \n",
    "    num_bootstrap_samples=10000, \n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b864748-1f1c-4f5b-9e01-ad04803dd242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(bootstrap_samples), bootstrap_samples[0][0].shape, bootstrap_samples[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f00d63-a8e5-4f5c-b5d5-ab60db894131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bootstrap_metric_ci(bootstrap_samples, y_true, y_pred, confidence_level=0.95, **kwargs):\n",
    "    n = len(bootstrap_samples)\n",
    "    \n",
    "    bootstrap_metrics = {\n",
    "        \"accuracy\": [],\n",
    "        \"f1_macro\": [],\n",
    "        \"f1_weighted\": [],\n",
    "        \"roc_auc_macro\": [],\n",
    "        \"roc_auc_weighted\": [],\n",
    "        \"classification_report_calc\": [],\n",
    "        \"pr_auc_macro\": [],\n",
    "        \"pr_auc_weighted\": [],\n",
    "        \"classwise_aucs\": {i: [] for i in range(7)},\n",
    "        \"classwise_pr_aucs\": {i: [] for i in range(7)}\n",
    "    }\n",
    "    \n",
    "    for sample_y_true, sample_y_pred in tqdm(bootstrap_samples):\n",
    "        # Calculate the macro and weighted ROC AUC\n",
    "        y_true_binarized = label_binarize(sample_y_true, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "        roc_auc_macro = roc_auc_score(y_true_binarized, sample_y_pred, multi_class='ovr', average='macro')\n",
    "        roc_auc_weighted = roc_auc_score(y_true_binarized, sample_y_pred, multi_class='ovr', average='weighted')\n",
    "        classification_report_calc = classification_report( sample_y_true, np.argmax( sample_y_pred, axis=1), output_dict=True )\n",
    "        \n",
    "        bootstrap_metrics[\"roc_auc_macro\"].append(roc_auc_macro)\n",
    "        bootstrap_metrics[\"roc_auc_weighted\"].append(roc_auc_weighted)\n",
    "        bootstrap_metrics['classification_report_calc'].append(classification_report_calc)\n",
    "\n",
    "        # Calculate the macro and weighted Precision-Recall AUC\n",
    "        pr_auc_macro = average_precision_score(y_true_binarized, sample_y_pred, average='macro')\n",
    "        pr_auc_weighted = average_precision_score(y_true_binarized, sample_y_pred, average='weighted')\n",
    "        bootstrap_metrics[\"pr_auc_macro\"].append(pr_auc_macro)\n",
    "        bootstrap_metrics[\"pr_auc_weighted\"].append(pr_auc_weighted)\n",
    "\n",
    "        # Convert probabilities to discrete class labels for F1-score and accuracy calculation\n",
    "        sample_y_pred_labels = np.argmax(sample_y_pred, axis=1)\n",
    "        \n",
    "        f1_macro = f1_score(sample_y_true, sample_y_pred_labels, average='macro')\n",
    "        f1_weighted = f1_score(sample_y_true, sample_y_pred_labels, average='weighted')\n",
    "        accuracy = accuracy_score(sample_y_true, sample_y_pred_labels)\n",
    "        bootstrap_metrics[\"f1_macro\"].append(f1_macro)\n",
    "        bootstrap_metrics[\"f1_weighted\"].append(f1_weighted)\n",
    "        bootstrap_metrics[\"accuracy\"].append(accuracy)\n",
    "        \n",
    "        # Calculate one-vs-rest AUC for each class (ROC AUC)\n",
    "        for class_index in range(7):\n",
    "            binarized_y_true = (sample_y_true == class_index).astype(int)\n",
    "            fpr, tpr, _ = roc_curve(binarized_y_true, sample_y_pred[:, class_index])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            bootstrap_metrics[\"classwise_aucs\"][class_index].append(roc_auc)\n",
    "\n",
    "            # Calculate one-vs-rest AUC for each class (Precision-Recall AUC)\n",
    "            precision, recall, _ = precision_recall_curve(binarized_y_true, sample_y_pred[:, class_index])\n",
    "            pr_auc = auc(recall, precision)\n",
    "            bootstrap_metrics[\"classwise_pr_aucs\"][class_index].append(pr_auc)\n",
    "\n",
    "    # Compute single metric values on the full dataset\n",
    "    y_true_binarized = label_binarize(y_true, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "    roc_auc_macro = roc_auc_score(y_true_binarized, y_pred, multi_class='ovr', average='macro')\n",
    "    roc_auc_weighted = roc_auc_score(y_true_binarized, y_pred, multi_class='ovr', average='weighted')\n",
    "    pr_auc_macro = average_precision_score(y_true_binarized, y_pred, average='macro')\n",
    "    pr_auc_weighted = average_precision_score(y_true_binarized, y_pred, average='weighted')\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)    \n",
    "    \n",
    "    f1_macro = f1_score(y_true, y_pred_labels, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred_labels, average='weighted')\n",
    "    accuracy = accuracy_score(y_true, y_pred_labels)\n",
    "\n",
    "    single_metric_values = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"roc_auc_macro\": roc_auc_macro,\n",
    "        \"roc_auc_weighted\": roc_auc_weighted,\n",
    "        \"pr_auc_macro\": pr_auc_macro,\n",
    "        \"pr_auc_weighted\": pr_auc_weighted,\n",
    "        \"classwise_aucs\": {},\n",
    "        \"classwise_pr_aucs\": {}\n",
    "    }\n",
    "\n",
    "    for class_index in range(7):\n",
    "        binarized_y_true = (y_true == class_index).astype(int)\n",
    "        fpr, tpr, _ = roc_curve(binarized_y_true, y_pred[:, class_index])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        single_metric_values[\"classwise_aucs\"][class_index] = roc_auc\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(binarized_y_true, y_pred[:, class_index])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        single_metric_values[\"classwise_pr_aucs\"][class_index] = pr_auc\n",
    "\n",
    "    # Compute confidence intervals\n",
    "    metric_cis = {}\n",
    "    for metric_name in [\"accuracy\", \"f1_macro\", \"f1_weighted\", \"roc_auc_macro\", \"roc_auc_weighted\", \"pr_auc_macro\", \"pr_auc_weighted\"]:\n",
    "        lower_bound = np.percentile(bootstrap_metrics[metric_name], (1 - confidence_level) / 2 * 100)\n",
    "        upper_bound = np.percentile(bootstrap_metrics[metric_name], (1 + confidence_level) / 2 * 100)\n",
    "        metric_cis[metric_name] = (single_metric_values[metric_name], lower_bound, upper_bound)\n",
    "\n",
    "    classwise_roc_cis = {}\n",
    "    classwise_pr_cis = {}\n",
    "    for class_index in range(7):\n",
    "        lower_bound_roc = np.percentile(bootstrap_metrics[\"classwise_aucs\"][class_index], (1 - confidence_level) / 2 * 100)\n",
    "        upper_bound_roc = np.percentile(bootstrap_metrics[\"classwise_aucs\"][class_index], (1 + confidence_level) / 2 * 100)\n",
    "        classwise_roc_cis[f\"Class {class_index} AUC\"] = (\n",
    "            single_metric_values[\"classwise_aucs\"][class_index], lower_bound_roc, upper_bound_roc\n",
    "        )\n",
    "\n",
    "        lower_bound_pr = np.percentile(bootstrap_metrics[\"classwise_pr_aucs\"][class_index], (1 - confidence_level) / 2 * 100)\n",
    "        upper_bound_pr = np.percentile(bootstrap_metrics[\"classwise_pr_aucs\"][class_index], (1 + confidence_level) / 2 * 100)\n",
    "        classwise_pr_cis[f\"Class {class_index} PR AUC\"] = (\n",
    "            single_metric_values[\"classwise_pr_aucs\"][class_index], lower_bound_pr, upper_bound_pr\n",
    "        )\n",
    "\n",
    "    return metric_cis, classwise_roc_cis, classwise_pr_cis, bootstrap_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314a7f9-5fc6-454a-9187-127228a28421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_cis, classwise_roc_cis, classwise_pr_cis, bootstrap_metrics = bootstrap_metric_ci(\n",
    "    bootstrap_samples, \n",
    "    y_true, \n",
    "    final_pred_ensemble_local_test\n",
    ")\n",
    "\n",
    "# Rename class enumeration to labels\n",
    "classwise_roc_cis = {f'{lb.classes_[i]} AUC': classwise_roc_cis[f'Class {i} AUC'] for i in range(len(lb.classes_))}\n",
    "classwise_pr_cis = {f'{lb.classes_[i]} AUC': classwise_pr_cis[f'Class {i} PR AUC'] for i in range(len(lb.classes_))}\n",
    "\n",
    "# Display single metric values and bootstrap results for the internal test set\n",
    "print(\"Internal test set (local test) - Single Metric Values and Bootstrap CIs:\")\n",
    "for metric_name, (value, lower, upper) in metric_cis.items():\n",
    "    print(f\"{metric_name:<40}: {round(value, 3)}, 95% CI = [{round(lower, 3)}, {round(upper, 3)}]\")\n",
    "\n",
    "print(\"\\nOne-vs-Rest Class-wise AUCs and Bootstrap CIs (ROC):\")\n",
    "for class_name, (value, lower, upper) in classwise_roc_cis.items():\n",
    "    print(f\"{class_name:<40}: {round(value, 3)}, 95% CI = [{round(lower, 3)}, {round(upper, 3)}]\")\n",
    "\n",
    "print(\"\\nOne-vs-Rest Class-wise AUCs and Bootstrap CIs (PR):\")\n",
    "for class_name, (value, lower, upper) in classwise_pr_cis.items():\n",
    "    print(f\"{class_name:<40}: {round(value, 3)}, 95% CI = [{round(lower, 3)}, {round(upper, 3)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dbd70-2bca-4433-9de5-ecb173103792",
   "metadata": {},
   "source": [
    "### Parse sklearn classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb733e4-cebd-4734-8aaa-0ce4d18df2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci_for_classification_report(bootstrap_metrics, lb_classes, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Compute confidence intervals and mean values for precision, recall, and f1-scores across all classes.\n",
    "    \n",
    "    Parameters:\n",
    "    bootstrap_metrics: list of dicts\n",
    "        List of classification report dictionaries generated from multiple bootstrap iterations.\n",
    "    lb_classes: list or array\n",
    "        The list or array containing the class names corresponding to the class indices.\n",
    "    confidence_level: float\n",
    "        Confidence level for the intervals (default is 0.95 for 95% CI).\n",
    "        \n",
    "    Returns:\n",
    "    ci_results: dict\n",
    "        Dictionary containing mean values and confidence intervals for each class and overall metrics.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store metrics for all classes and overall metrics\n",
    "    num_classes = len(lb_classes)\n",
    "    all_metrics = {\n",
    "        \"precision\": {lb_classes[i]: [] for i in range(num_classes)},  # Use class names instead of indices\n",
    "        \"recall\": {lb_classes[i]: [] for i in range(num_classes)},\n",
    "        \"f1_score\": {lb_classes[i]: [] for i in range(num_classes)},\n",
    "        \"precision_macro\": [],\n",
    "        \"recall_macro\": [],\n",
    "        \"f1_macro\": [],\n",
    "        \"precision_weighted\": [],\n",
    "        \"recall_weighted\": [],\n",
    "        \"f1_weighted\": [],\n",
    "        \"accuracy\": []\n",
    "    }\n",
    "\n",
    "    # Parse each classification report from the bootstrap iterations\n",
    "    for report in bootstrap_metrics['classification_report_calc']:\n",
    "        # For each class, map by lb_classes\n",
    "        for class_id in range(num_classes):\n",
    "            class_str = lb_classes[class_id]  # Get class name\n",
    "            all_metrics[\"precision\"][class_str].append(report[str(class_id)][\"precision\"])\n",
    "            all_metrics[\"recall\"][class_str].append(report[str(class_id)][\"recall\"])\n",
    "            all_metrics[\"f1_score\"][class_str].append(report[str(class_id)][\"f1-score\"])\n",
    "        \n",
    "        # Add macro and weighted average values\n",
    "        all_metrics[\"precision_macro\"].append(report[\"macro avg\"][\"precision\"])\n",
    "        all_metrics[\"recall_macro\"].append(report[\"macro avg\"][\"recall\"])\n",
    "        all_metrics[\"f1_macro\"].append(report[\"macro avg\"][\"f1-score\"])\n",
    "        all_metrics[\"precision_weighted\"].append(report[\"weighted avg\"][\"precision\"])\n",
    "        all_metrics[\"recall_weighted\"].append(report[\"weighted avg\"][\"recall\"])\n",
    "        all_metrics[\"f1_weighted\"].append(report[\"weighted avg\"][\"f1-score\"])\n",
    "        all_metrics[\"accuracy\"].append(report[\"accuracy\"])\n",
    "\n",
    "    # Function to calculate mean and confidence intervals for a list of values\n",
    "    def calculate_mean_and_ci(data, confidence_level):\n",
    "        mean_value = np.mean(data)\n",
    "        lower_bound = np.percentile(data, (1 - confidence_level) / 2 * 100)\n",
    "        upper_bound = np.percentile(data, (1 + confidence_level) / 2 * 100)\n",
    "        return (mean_value, lower_bound, upper_bound)\n",
    "\n",
    "    # Initialize a dictionary to store the CI results\n",
    "    ci_results = {}\n",
    "\n",
    "    # Compute CI and mean for each class (precision, recall, f1-score)\n",
    "    for class_id in range(num_classes):\n",
    "        class_str = lb_classes[class_id]  # Get class name\n",
    "        ci_results[class_str] = {\n",
    "            \"precision\": calculate_mean_and_ci(all_metrics[\"precision\"][class_str], confidence_level),\n",
    "            \"recall\": calculate_mean_and_ci(all_metrics[\"recall\"][class_str], confidence_level),\n",
    "            \"f1_score\": calculate_mean_and_ci(all_metrics[\"f1_score\"][class_str], confidence_level)\n",
    "        }\n",
    "\n",
    "    # Compute CI and mean for overall metrics (macro avg, weighted avg, and accuracy)\n",
    "    ci_results[\"macro_avg\"] = {\n",
    "        \"precision\": calculate_mean_and_ci(all_metrics[\"precision_macro\"], confidence_level),\n",
    "        \"recall\": calculate_mean_and_ci(all_metrics[\"recall_macro\"], confidence_level),\n",
    "        \"f1_score\": calculate_mean_and_ci(all_metrics[\"f1_macro\"], confidence_level)\n",
    "    }\n",
    "    ci_results[\"weighted_avg\"] = {\n",
    "        \"precision\": calculate_mean_and_ci(all_metrics[\"precision_weighted\"], confidence_level),\n",
    "        \"recall\": calculate_mean_and_ci(all_metrics[\"recall_weighted\"], confidence_level),\n",
    "        \"f1_score\": calculate_mean_and_ci(all_metrics[\"f1_weighted\"], confidence_level)\n",
    "    }\n",
    "    ci_results[\"accuracy\"] = calculate_mean_and_ci(all_metrics[\"accuracy\"], confidence_level)\n",
    "\n",
    "    return ci_results\n",
    "\n",
    "def print_ci_results_with_original(ci_results, original_report, lb_classes):\n",
    "    \"\"\"\n",
    "    Print the CI results for precision, recall, and f1-score, replacing the mean values with\n",
    "    the ones from the original classification report. It aligns class names between the original report and ci_results.\n",
    "    \n",
    "    Parameters:\n",
    "    ci_results: dict\n",
    "        Dictionary containing confidence intervals for each class and overall metrics.\n",
    "    original_report: dict\n",
    "        Original classification report (non-bootstrapped), used to replace the mean values in the output.\n",
    "    lb_classes: list or array\n",
    "        List of class names to map the original report classes to the same as ci_results.\n",
    "    \"\"\"\n",
    "    # Align class names in the original report to match ci_results' format\n",
    "    mapped_original_report = {}\n",
    "    for idx, class_name in enumerate(lb_classes):\n",
    "        mapped_original_report[class_name] = original_report[str(idx)]\n",
    "\n",
    "    # Add macro_avg and weighted_avg back if they are present in original report\n",
    "    mapped_original_report[\"macro_avg\"] = original_report[\"macro avg\"]\n",
    "    mapped_original_report[\"weighted_avg\"] = original_report[\"weighted avg\"]\n",
    "    mapped_original_report[\"accuracy\"] = original_report[\"accuracy\"]\n",
    "\n",
    "    print(\"One-vs-Rest Class-wise Metrics and Bootstrap CIs:\")\n",
    "\n",
    "    # Loop through the classes in ci_results and print the metrics\n",
    "    for class_name, metrics in ci_results.items():\n",
    "        if class_name in [\"macro_avg\", \"weighted_avg\", \"accuracy\"]:\n",
    "            continue  # Skipping overall averages for this printout\n",
    "\n",
    "        print(f\"\\n{class_name.upper()} Metrics:\")\n",
    "        for metric_name, (mean_value, lower_bound, upper_bound) in metrics.items():\n",
    "            # Use 'f1-score' instead of 'f1_score' for the original report\n",
    "            metric_name_in_original = metric_name if metric_name != 'f1_score' else 'f1-score'\n",
    "            # Replace the mean_value with the value from the mapped original classification report\n",
    "            original_mean_value = mapped_original_report[class_name][metric_name_in_original]\n",
    "            print(f\"  {metric_name.capitalize().replace('_', '-')} : {original_mean_value:.3f}, 95% CI = [{lower_bound:.3f}, {upper_bound:.3f}]\")\n",
    "\n",
    "    # Print the macro and weighted averages separately\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for avg_type in [\"macro_avg\", \"weighted_avg\"]:\n",
    "        print(f\"\\n{avg_type.replace('_', ' ').capitalize()}:\")\n",
    "        for metric_name, (mean_value, lower_bound, upper_bound) in ci_results[avg_type].items():\n",
    "            metric_name_in_original = metric_name if metric_name != 'f1_score' else 'f1-score'\n",
    "            # Replace the mean_value with the value from the original classification report\n",
    "            original_mean_value = mapped_original_report[avg_type][metric_name_in_original]\n",
    "            print(f\"  {metric_name.capitalize().replace('_', '-')} : {original_mean_value:.3f}, 95% CI = [{lower_bound:.3f}, {upper_bound:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828042a-f5e9-4db5-a10e-b07a76c9fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report( y_val_oh, np.argmax( bracs_class_pred_val_all, axis=1), digits=3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519b564-f922-43f7-8dbd-4b29f5698e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reports_ci_results = compute_ci_for_classification_report(bootstrap_metrics, lb.classes_, confidence_level=0.95)\n",
    "print_ci_results_with_original(class_reports_ci_results, classification_report( \n",
    "    y_val_oh, \n",
    "    np.argmax( bracs_class_pred_val_all, axis=1), digits=3, output_dict=True ), lb.classes_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee7d74-bc6a-4d29-b0f1-0ccd8874af65",
   "metadata": {},
   "source": [
    "### ROC curve with CI - simple interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b4ea1-70c1-49df-9f02-260c4f782cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_bootstrap_roc_curve(bootstrap_samples, class_index):\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "\n",
    "    for sample_y_true, sample_y_pred in tqdm(bootstrap_samples):\n",
    "        # Binarize the true labels for one-vs-rest classification\n",
    "        binarized_y_true = (sample_y_true == class_index).astype(int)\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(binarized_y_true, sample_y_pred[:, class_index])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        # Interpolate the TPR values to get consistent x-axis (FPR) values\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std_tprs = tprs.std(axis=0)\n",
    "    \n",
    "    tpr_lower = np.percentile(tprs, 2.5, axis=0)\n",
    "    tpr_upper = np.percentile(tprs, 97.5, axis=0)\n",
    "\n",
    "    # Calculate 95% CI for the AUC values\n",
    "    auc_lower = np.percentile(aucs, 2.5)\n",
    "    auc_upper = np.percentile(aucs, 97.5)\n",
    "    \n",
    "    return base_fpr, tpr_lower, tpr_upper, mean_tprs, auc_lower, auc_upper"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a8f77b8-2347-4b9f-b51b-8620291cfbde",
   "metadata": {},
   "source": [
    "!mkdir paper_figures"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4f77b72-4c74-4ddf-879c-266a9ec0fe0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "def plot_roc_with_saved_bootstrap(bootstrap_samples, y_true, y_pred):\n",
    "    if y_pred.shape[1] != 7:\n",
    "        raise ValueError(\"The number of classes should be 4\")\n",
    "\n",
    "    if y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"Mismatched shape between y_true and y_pred\")\n",
    "\n",
    "    # One-hot encode y_true if necessary\n",
    "    if len(y_true.shape) == 1 or y_true.shape[1] != 7:\n",
    "        y_true = F.one_hot(torch.from_numpy(y_true).to(torch.int64), 7).numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 7, figsize=(42, 6), dpi=150)\n",
    "\n",
    "    for class_ind in range(y_pred.shape[1]):\n",
    "        # Calculate the original ROC curve and AUC for the given class\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Calculate bootstrap-based ROC curves and CIs using saved samples\n",
    "        base_fpr, tpr_lower, tpr_upper, mean_tprs, auc_lower, auc_upper = calculate_bootstrap_roc_curve(bootstrap_samples, class_ind)\n",
    "\n",
    "        axs[class_ind].plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})', color='blue', lw=2)\n",
    "        if len(tpr_lower) > 0 and len(tpr_upper) > 0:  # Check if CI is computed\n",
    "            axs[class_ind].fill_between(base_fpr, tpr_lower, tpr_upper, color='lightblue', alpha=0.2, label=f'95% CI [{auc_lower:.3f}, {auc_upper:.3f}]')\n",
    "        else:\n",
    "            print(\"CI NOT COMPUTED\")\n",
    "        \n",
    "        ## plot no skill line\n",
    "        axs[class_ind].plot([0, 1], [0, 1], linestyle='dashed', lw=2, color='black', label=\"No skill classifier\")        \n",
    "        \n",
    "        axs[class_ind].axis(\"square\")\n",
    "        axs[class_ind].set_xlim([-0.05, 1.05])\n",
    "        axs[class_ind].set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        axs[class_ind].set_xlabel('False Positive Rate', fontsize=22)\n",
    "        axs[class_ind].set_ylabel('True Positive Rate', fontsize=22)\n",
    "\n",
    "        stage_names = lb.classes_ # [\"I\", \"II\", \"III\", \"IV\"]\n",
    "        axs[class_ind].set_title(f'{stage_names[class_ind]}', fontsize=22, pad=20)\n",
    "        axs[class_ind].legend(loc='lower right', fontsize=16)\n",
    "        \n",
    "        # Set ticks for both axes\n",
    "        axs[class_ind].set_xticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[class_ind].set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[class_ind].tick_params(axis='both', which='major', labelsize=22)\n",
    "        axs[class_ind].xaxis.label.set_size(22)\n",
    "        axs[class_ind].yaxis.label.set_size(22)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paper_figures/roc_curve_with_ci_bracs_test_uni.png')\n",
    "    plt.savefig('paper_figures/roc_curve_with_ci_bracs_test_uni.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48359fdc-1887-4cb9-b875-09acc30df9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_with_saved_bootstrap(bootstrap_samples, y_true, y_pred):\n",
    "    if y_pred.shape[1] != 7:\n",
    "        raise ValueError(\"The number of classes should be 7\")\n",
    "\n",
    "    if y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"Mismatched shape between y_true and y_pred\")\n",
    "\n",
    "    # One-hot encode y_true if necessary\n",
    "    if len(y_true.shape) == 1 or y_true.shape[1] != 7:\n",
    "        y_true = F.one_hot(torch.from_numpy(y_true).to(torch.int64), 7).numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 7, figsize=(42, 6), dpi=150)\n",
    "\n",
    "    # Reorder indices according to desired abbreviations\n",
    "    lb_classes_abbr = [reverse_mapping[cls] for cls in lb_classes]\n",
    "    sorted_indices = np.argsort([desired_order.index(abbr) for abbr in lb_classes_abbr])\n",
    "\n",
    "    for i, class_ind in enumerate(sorted_indices):\n",
    "        # Calculate the original ROC curve and AUC for the given class\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Calculate bootstrap-based ROC curves and CIs using saved samples\n",
    "        base_fpr, tpr_lower, tpr_upper, mean_tprs, auc_lower, auc_upper = calculate_bootstrap_roc_curve(bootstrap_samples, class_ind)\n",
    "\n",
    "        axs[i].plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})', color='blue', lw=2)\n",
    "        if len(tpr_lower) > 0 and len(tpr_upper) > 0:  # Check if CI is computed\n",
    "            axs[i].fill_between(base_fpr, tpr_lower, tpr_upper, color='lightblue', alpha=0.2, label=f'95% CI [{auc_lower:.3f}, {auc_upper:.3f}]')\n",
    "        else:\n",
    "            print(\"CI NOT COMPUTED\")\n",
    "        \n",
    "        # Plot no skill line\n",
    "        axs[i].plot([0, 1], [0, 1], linestyle='dashed', lw=2, color='black', label=\"No skill classifier\")        \n",
    "        \n",
    "        axs[i].axis(\"square\")\n",
    "        axs[i].set_xlim([-0.05, 1.05])\n",
    "        axs[i].set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        axs[i].set_xlabel('False Positive Rate', fontsize=22)\n",
    "        axs[i].set_ylabel('True Positive Rate', fontsize=22)\n",
    "\n",
    "        # Set the title according to the reordered stage names\n",
    "        stage_names = lb_classes_abbr  # use the original class names\n",
    "        axs[i].set_title(f'{stage_names[class_ind]}', fontsize=22, pad=20)\n",
    "        axs[i].legend(loc='lower right', fontsize=16)\n",
    "        \n",
    "        # Set ticks for both axes\n",
    "        axs[i].set_xticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[i].set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=22)\n",
    "        axs[i].xaxis.label.set_size(22)\n",
    "        axs[i].yaxis.label.set_size(22)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paper_figures/roc_curve_with_ci_bracs_test_resnet50_simplehead.png')\n",
    "    plt.savefig('paper_figures/roc_curve_with_ci_bracs_test_resnet50_simplehead.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a29993-9f0d-4aa1-8869-a1c1d000d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_with_saved_bootstrap(bootstrap_samples, biopsy_df_local_test, final_pred_ensemble_local_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a2c87-d164-49a3-9e81-229a808782e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PR curve with CI - simple interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f5235-3aad-4267-9194-f09d9f4b8109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_bootstrap_pr_curve_simple_interp(bootstrap_samples, class_index):\n",
    "    base_recalls = np.linspace(0, 1, 101)\n",
    "    precisions = []\n",
    "    pr_aucs = []\n",
    "\n",
    "    for sample_y_true, sample_y_pred in tqdm(bootstrap_samples):\n",
    "        # Binarize the true labels for one-vs-rest classification\n",
    "        binarized_y_true = (sample_y_true == class_index).astype(int)\n",
    "        \n",
    "        # Calculate precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(binarized_y_true, sample_y_pred[:, class_index])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        pr_aucs.append(pr_auc)\n",
    "\n",
    "        precision_interp = np.interp(base_recalls, recall[::-1], precision[::-1])\n",
    "        precisions.append(precision_interp)\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    mean_precisions = precisions.mean(axis=0)\n",
    "    std_precisions = precisions.std(axis=0)\n",
    "    \n",
    "    precision_lower = np.percentile(precisions, 2.5, axis=0)\n",
    "    precision_upper = np.percentile(precisions, 97.5, axis=0)\n",
    "\n",
    "    # Calculate 95% CI for the PR AUC values\n",
    "    pr_auc_lower = np.percentile(pr_aucs, 2.5)\n",
    "    pr_auc_upper = np.percentile(pr_aucs, 97.5)\n",
    "    \n",
    "    return base_recalls, precision_lower, precision_upper, mean_precisions, pr_auc_lower, pr_auc_upper"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbbe390e-e723-4441-8867-defaaab3c99b",
   "metadata": {
    "tags": []
   },
   "source": [
    "def plot_pr_with_saved_bootstrap_simple_interp(bootstrap_samples, y_true, y_pred):\n",
    "    if y_pred.shape[1] != 7:\n",
    "        raise ValueError(\"The number of classes should be 4\")\n",
    "\n",
    "    if y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"Mismatched shape between y_true and y_pred\")\n",
    "\n",
    "    # One-hot encode y_true if necessary\n",
    "    if len(y_true.shape) == 1 or y_true.shape[1] != 7:\n",
    "        y_true = np.eye(7)[y_true]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 7, figsize=(42, 6), dpi=150)\n",
    "\n",
    "    for class_ind in range(y_pred.shape[1]):\n",
    "        # Calculate the original precision-recall curve and AUC for the given class\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Calculate bootstrap-based precision-recall curves and CIs using saved samples\n",
    "        base_recalls, precision_lower, precision_upper, mean_precisions, pr_auc_lower, pr_auc_upper = calculate_bootstrap_pr_curve_simple_interp(bootstrap_samples, class_ind)\n",
    "\n",
    "        axs[class_ind].plot(recall, precision, label=f'PR curve (AP = {pr_auc:.3f})', color='green', lw=2)\n",
    "        if len(precision_lower) > 0 and len(precision_upper) > 0:  # Check if CI is computed\n",
    "            axs[class_ind].fill_between(base_recalls, precision_lower, precision_upper, color='lightgreen', alpha=0.2, label=f'95% CI [{pr_auc_lower:.3f}, {pr_auc_upper:.3f}]')\n",
    "        else:\n",
    "            print(\"CI NOT COMPUTED\")\n",
    "            \n",
    "        ## plot no skill line\n",
    "        axs[class_ind].hlines(\n",
    "            y_true[:, class_ind].sum() / y_true[:, class_ind].shape[0], \n",
    "            0,\n",
    "            1, \n",
    "            color=\"black\", \n",
    "            label=\"No skill classifier\",\n",
    "            lw=2,\n",
    "            linestyles='dashed',\n",
    "            alpha=1.)\n",
    "\n",
    "        axs[class_ind].axis(\"square\")\n",
    "        axs[class_ind].set_xlim([-0.05, 1.05])\n",
    "        axs[class_ind].set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        axs[class_ind].set_xlabel('Recall', fontsize=22)\n",
    "        axs[class_ind].set_ylabel('Precision', fontsize=22)\n",
    "\n",
    "        stage_names = lb.classes_ #[\"I\", \"II\", \"III\", \"IV\"]\n",
    "        axs[class_ind].set_title(f'{stage_names[class_ind]}', fontsize=22, pad=20)\n",
    "        axs[class_ind].legend(fontsize=16)\n",
    "        \n",
    "        # Set ticks for both axes\n",
    "        axs[class_ind].set_xticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[class_ind].set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[class_ind].tick_params(axis='both', which='major', labelsize=22)\n",
    "        axs[class_ind].xaxis.label.set_size(22)\n",
    "        axs[class_ind].yaxis.label.set_size(22)\n",
    "        \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paper_figures/pr_curve_with_ci_bracs_test_uni.png')\n",
    "    plt.savefig('paper_figures/pr_curve_with_ci_bracs_test_uni.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc4c23-2b7d-462c-be6a-c065df644dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_with_saved_bootstrap_simple_interp(bootstrap_samples, y_true, y_pred):\n",
    "    if y_pred.shape[1] != 7:\n",
    "        raise ValueError(\"The number of classes should be 7\")\n",
    "\n",
    "    if y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError(\"Mismatched shape between y_true and y_pred\")\n",
    "\n",
    "    # One-hot encode y_true if necessary\n",
    "    if len(y_true.shape) == 1 or y_true.shape[1] != 7:\n",
    "        y_true = np.eye(7)[y_true]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 7, figsize=(42, 6), dpi=150)\n",
    "\n",
    "    # Reorder indices according to desired abbreviations\n",
    "    lb_classes_abbr = [reverse_mapping[cls] for cls in lb_classes]\n",
    "    sorted_indices = np.argsort([desired_order.index(abbr) for abbr in lb_classes_abbr])\n",
    "\n",
    "    for i, class_ind in enumerate(sorted_indices):\n",
    "        # Calculate the original precision-recall curve and AUC for the given class\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, class_ind], y_pred[:, class_ind])\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Calculate bootstrap-based precision-recall curves and CIs using saved samples\n",
    "        base_recalls, precision_lower, precision_upper, mean_precisions, pr_auc_lower, pr_auc_upper = calculate_bootstrap_pr_curve_simple_interp(bootstrap_samples, class_ind)\n",
    "\n",
    "        axs[i].plot(recall, precision, label=f'PR curve (AP = {pr_auc:.3f})', color='green', lw=2)\n",
    "        if len(precision_lower) > 0 and len(precision_upper) > 0:  # Check if CI is computed\n",
    "            axs[i].fill_between(base_recalls, precision_lower, precision_upper, color='lightgreen', alpha=0.2, label=f'95% CI [{pr_auc_lower:.3f}, {pr_auc_upper:.3f}]')\n",
    "        else:\n",
    "            print(\"CI NOT COMPUTED\")\n",
    "            \n",
    "        # Plot no skill line\n",
    "        axs[i].hlines(\n",
    "            y_true[:, class_ind].sum() / y_true[:, class_ind].shape[0], \n",
    "            0,\n",
    "            1, \n",
    "            color=\"black\", \n",
    "            label=\"No skill classifier\",\n",
    "            lw=2,\n",
    "            linestyles='dashed',\n",
    "            alpha=1.)\n",
    "\n",
    "        axs[i].axis(\"square\")\n",
    "        axs[i].set_xlim([-0.05, 1.05])\n",
    "        axs[i].set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        axs[i].set_xlabel('Recall', fontsize=22)\n",
    "        axs[i].set_ylabel('Precision', fontsize=22)\n",
    "\n",
    "        # Set the title according to the reordered stage names\n",
    "        stage_names = lb_classes_abbr  # use the original class names\n",
    "        axs[i].set_title(f'{stage_names[class_ind]}', fontsize=22, pad=20)\n",
    "        axs[i].legend(fontsize=16)\n",
    "        \n",
    "        # Set ticks for both axes\n",
    "        axs[i].set_xticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[i].set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=22)\n",
    "        axs[i].xaxis.label.set_size(22)\n",
    "        axs[i].yaxis.label.set_size(22)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paper_figures/pr_curve_with_ci_bracs_test_resnet50_simplehead.png')\n",
    "    plt.savefig('paper_figures/pr_curve_with_ci_bracs_test_resnet50_simplehead.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d65dc-a15b-4f98-b8ad-2bd7e881c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_with_saved_bootstrap_simple_interp(bootstrap_samples, biopsy_df_local_test, final_pred_ensemble_local_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f89939-b985-4dbb-8460-bb92a64ff4c1",
   "metadata": {},
   "source": [
    "### Confusion matrix - default argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54990a01-14ca-4596-8f0a-63f7aa5a16d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculating the confusion matrix\n",
    "confusion_matrix_initial = pd.crosstab( biopsy_df_local_test, np.argmax( final_pred_ensemble_local_test, axis=1),\n",
    "                                rownames=['True stages'], colnames=['Predicted stage'] )\n",
    "\n",
    "confusion_matrix_padded = confusion_matrix_initial.reindex(index=range(0, 7), columns=range(0, 7), fill_value=0)\n",
    "\n",
    "custom_heatmap = np.round(confusion_matrix_initial,0).astype(int).astype(str) #+ '  ' + std_confusion_matrix_tuned.round(0).astype(int).astype(str)\n",
    "\n",
    "\n",
    "# visualizng it on a heatmap\n",
    "plt.figure(figsize=(6,6), dpi=100)\n",
    "plt.title( 'Confusion matrix of MIL')\n",
    "sns.heatmap(confusion_matrix_padded, annot=True, fmt='.5g', \n",
    "            xticklabels=np.array(lb.classes_), \n",
    "            yticklabels=np.array(lb.classes_), annot_kws={\"size\": 14}\n",
    "           )\n",
    "plt.tick_params(labelsize=12, rotation=25)\n",
    "plt.xlabel('Predicted stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75365d66-f6c1-499b-8430-3566abf71f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix (example code)\n",
    "confusion_matrix_initial = pd.crosstab(\n",
    "    biopsy_df_local_test, \n",
    "    np.argmax(final_pred_ensemble_local_test, axis=1),\n",
    "    rownames=['True stages'], colnames=['Predicted stage']\n",
    ")\n",
    "\n",
    "# Reindex the confusion matrix based on sorted indices\n",
    "confusion_matrix_padded = confusion_matrix_initial.reindex(index=sorted_indices, columns=sorted_indices, fill_value=0)\n",
    "\n",
    "# Creating a custom heatmap\n",
    "custom_heatmap = np.round(confusion_matrix_padded, 0).astype(int).astype(str)\n",
    "\n",
    "# Visualizing the heatmap with reordered x and y labels\n",
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.title('Confusion matrix of MIL')\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix_padded, \n",
    "    annot=True, \n",
    "    fmt='.5g', \n",
    "    xticklabels=desired_order, \n",
    "    yticklabels=desired_order, \n",
    "    annot_kws={\"size\": 14}\n",
    ")\n",
    "\n",
    "plt.tick_params(labelsize=12, rotation=25)\n",
    "plt.xlabel('Predicted stage')\n",
    "plt.ylabel('True stage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cb788f8-1d2f-43a2-8659-21a012e0a48b",
   "metadata": {},
   "source": [
    "# Normalize confusion matrix (convert to numpy arrays first)\n",
    "confusion_matrix_normalized = confusion_matrix_initial.values.astype('float') / confusion_matrix_initial.values.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Replace NaN values with zeros (this happens where the sum of rows is zero, like in row 4)\n",
    "#mean_confusion_matrix_normalized = np.nan_to_num(mean_confusion_matrix_normalized)\n",
    "\n",
    "# Create custom annotations with percentages as main and actual values in parentheses\n",
    "custom_annotations = np.empty_like(confusion_matrix_initial.values, dtype=object)\n",
    "for i in range(confusion_matrix_initial.shape[0]):\n",
    "    for j in range(confusion_matrix_initial.shape[1]):\n",
    "        actual_value = int(confusion_matrix_initial.values[i, j])\n",
    "        percentage_value = confusion_matrix_normalized[i, j] \n",
    "        custom_annotations[i, j] = f\"{percentage_value:.2f}\\n({actual_value})\"\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi=150)\n",
    "\n",
    "# Heatmap with both percentages and actual values\n",
    "sns.heatmap(confusion_matrix_normalized, annot=custom_annotations, fmt=\"\", cmap='viridis', ax=ax, vmin=0, vmax=1, cbar_kws={\"shrink\": 0.65})\n",
    "ax.set_ylabel('True stages', fontsize=14)\n",
    "ax.set_xlabel('Predicted stages', fontsize=14)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_box_aspect(1)\n",
    "\n",
    "# Set custom tick labels for x and y axes\n",
    "ax.set_xticklabels(lb.classes_, rotation=90)\n",
    "ax.set_yticklabels(lb.classes_, rotation=0)\n",
    "\n",
    "# Add colorbar labels and fix size\n",
    "cbar = ax.collections[0].colorbar  # Access the colorbar\n",
    "cbar.set_label('Normalized scale', fontsize=14)\n",
    "cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "cbar.ax.tick_params(labelsize=10)  # Adjust tick size\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "#plt.savefig('paper_figures/confmat_tcga_test_combined_percentages_main_resnet50.png')\n",
    "#plt.savefig('paper_figures/confmat_tcga_test_combined_percentages_main_resnet50.svg', format='svg')\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babe9eb-bac2-4918-b21d-a00a75309b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting indices to reorder rows and columns of the confusion matrix\n",
    "sorted_indices = np.argsort([desired_order.index(abbr) for abbr in lb_classes_abbr])\n",
    "\n",
    "# Normalize confusion matrix (convert to numpy arrays first)\n",
    "confusion_matrix_normalized = confusion_matrix_initial.values.astype('float') / confusion_matrix_initial.values.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Reindex both the normalized confusion matrix and the original one for actual values\n",
    "confusion_matrix_normalized = confusion_matrix_initial.reindex(index=sorted_indices, columns=sorted_indices).values.astype('float') / confusion_matrix_initial.reindex(index=sorted_indices, columns=sorted_indices).values.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Replace NaN values with zeros (in case some rows have all zero values)\n",
    "confusion_matrix_normalized = np.nan_to_num(confusion_matrix_normalized)\n",
    "\n",
    "# Create custom annotations with percentages as main and actual values in parentheses\n",
    "custom_annotations = np.empty_like(confusion_matrix_normalized, dtype=object)\n",
    "for i in range(confusion_matrix_normalized.shape[0]):\n",
    "    for j in range(confusion_matrix_normalized.shape[1]):\n",
    "        actual_value = int(confusion_matrix_initial.reindex(index=sorted_indices, columns=sorted_indices).values[i, j])\n",
    "        percentage_value = confusion_matrix_normalized[i, j]\n",
    "        custom_annotations[i, j] = f\"{percentage_value:.2f}\\n({actual_value})\"\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), dpi=150)\n",
    "\n",
    "# Heatmap with both percentages and actual values\n",
    "sns.heatmap(confusion_matrix_normalized, annot=custom_annotations, fmt=\"\", cmap='viridis', ax=ax, vmin=0, vmax=1, cbar_kws={\"shrink\": 0.65})\n",
    "ax.set_ylabel('True stages', fontsize=14)\n",
    "ax.set_xlabel('Predicted stages', fontsize=14)\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_box_aspect(1)\n",
    "\n",
    "# Set custom tick labels for x and y axes to use abbreviations\n",
    "ax.set_xticklabels(desired_order, rotation=0)\n",
    "ax.set_yticklabels(desired_order, rotation=0)\n",
    "\n",
    "# Add colorbar labels and adjust size\n",
    "cbar = ax.collections[0].colorbar  # Access the colorbar\n",
    "cbar.set_label('Normalized scale', fontsize=14)\n",
    "cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "cbar.ax.tick_params(labelsize=10)  # Adjust tick size\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure (optional)\n",
    "plt.savefig('paper_figures/confmat_bracs_test_combined_percentages_resnet50_simplehead.png')\n",
    "plt.savefig('paper_figures/confmat_bracs_test_combined_percentages_resnet50_simplehead.svg', format='svg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0573b-99ec-46bd-89ef-1e404b820b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72663132-660e-4863-9215-c2cab7495996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
